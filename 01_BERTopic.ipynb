{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bertopic\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3032\\2937779436.py:3: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  publication = pd.read_csv('eu_pub_publication.csv')\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3032\\2937779436.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  institution = pd.read_csv('eu_pub_institution.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubid</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>quant_label</th>\n",
       "      <th>abstract</th>\n",
       "      <th>itemtitle</th>\n",
       "      <th>eu_nuts_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;1. The histamine H-2 receptor antagonis...</td>\n",
       "      <td>Pharmacology of JB-9315, a new selective hista...</td>\n",
       "      <td>ES415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;1. We have investigated the ability of ...</td>\n",
       "      <td>Focal cerebral ischemia in the mouse: Hypother...</td>\n",
       "      <td>NL327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;1. For several years we have been worki...</td>\n",
       "      <td>Pyridazine derivatives XIV. Study of the vasor...</td>\n",
       "      <td>ES111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;1. The structural and ionic requirement...</td>\n",
       "      <td>Structural requirements and ionic mechanism of...</td>\n",
       "      <td>UKJ36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;1. The effect of taxol on selected lyso...</td>\n",
       "      <td>Activity of lysosomal system in mouse liver af...</td>\n",
       "      <td>PL331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pubid  pubyear quant_label  \\\n",
       "0  2010.0     1998         NaN   \n",
       "1  2012.0     1998         NaN   \n",
       "2  2013.0     1998         NaN   \n",
       "3  2015.0     1998         NaN   \n",
       "4  2019.0     1998         NaN   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  \\r\\n<p>1. The histamine H-2 receptor antagonis...   \n",
       "1  \\r\\n<p>1. We have investigated the ability of ...   \n",
       "2  \\r\\n<p>1. For several years we have been worki...   \n",
       "3  \\r\\n<p>1. The structural and ionic requirement...   \n",
       "4  \\r\\n<p>1. The effect of taxol on selected lyso...   \n",
       "\n",
       "                                           itemtitle eu_nuts_id  \n",
       "0  Pharmacology of JB-9315, a new selective hista...      ES415  \n",
       "1  Focal cerebral ischemia in the mouse: Hypother...      NL327  \n",
       "2  Pyridazine derivatives XIV. Study of the vasor...      ES111  \n",
       "3  Structural requirements and ionic mechanism of...      UKJ36  \n",
       "4  Activity of lysosomal system in mouse liver af...      PL331  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset \n",
    "\n",
    "publication = pd.read_csv('eu_pub_publication.csv')\n",
    "institution = pd.read_csv('eu_pub_institution.csv')\n",
    "\n",
    "# drop uncessary columns\n",
    "publication = publication.drop(columns=['period'])\n",
    "institution = institution.drop(columns=['pubyear', 'quant_label', 'period'])\n",
    "institution = institution[['pubid', 'eu_nuts_id']]\n",
    "\n",
    "# drop duplicates\n",
    "institution = institution.drop_duplicates()\n",
    "\n",
    "# remove rows with missing values\n",
    "institution = institution.dropna()\n",
    "\n",
    "# merge the two datasets\n",
    "df = pd.merge(publication, institution, on='pubid')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubid</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>quant_label</th>\n",
       "      <th>abstract</th>\n",
       "      <th>itemtitle</th>\n",
       "      <th>eu_nuts_id</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5706114</th>\n",
       "      <td>7256411.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;Background: In the United Kingdom, heal...</td>\n",
       "      <td>The Formal Support Experiences of Mothers of A...</td>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5706115</th>\n",
       "      <td>7256419.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;Background: Understanding how male nurs...</td>\n",
       "      <td>The Influence of Personality Traits and Social...</td>\n",
       "      <td>UKF14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5706116</th>\n",
       "      <td>7256433.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;Background: Evidence suggests that olde...</td>\n",
       "      <td>The Experience of Applying a Narrative Researc...</td>\n",
       "      <td>UKN04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5706117</th>\n",
       "      <td>7256459.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;Background: Sex work is receiving incre...</td>\n",
       "      <td>Sex work and the 2010 FIFA World Cup: time for...</td>\n",
       "      <td>BE234</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5706118</th>\n",
       "      <td>7256460.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n&lt;p&gt;Sickle Cell Disorder is a global health...</td>\n",
       "      <td>Psychosocial impact of sickle cell disorder: p...</td>\n",
       "      <td>UKI72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pubid  pubyear quant_label  \\\n",
       "5706114  7256411.0     2010         NaN   \n",
       "5706115  7256419.0     2010         NaN   \n",
       "5706116  7256433.0     2010         NaN   \n",
       "5706117  7256459.0     2010         NaN   \n",
       "5706118  7256460.0     2010         NaN   \n",
       "\n",
       "                                                  abstract  \\\n",
       "5706114  \\r\\n<p>Background: In the United Kingdom, heal...   \n",
       "5706115  \\r\\n<p>Background: Understanding how male nurs...   \n",
       "5706116  \\r\\n<p>Background: Evidence suggests that olde...   \n",
       "5706117  \\r\\n<p>Background: Sex work is receiving incre...   \n",
       "5706118  \\r\\n<p>Sickle Cell Disorder is a global health...   \n",
       "\n",
       "                                                 itemtitle eu_nuts_id  period  \n",
       "5706114  The Formal Support Experiences of Mothers of A...      UKM25       1  \n",
       "5706115  The Influence of Personality Traits and Social...      UKF14       1  \n",
       "5706116  The Experience of Applying a Narrative Researc...      UKN04       1  \n",
       "5706117  Sex work and the 2010 FIFA World Cup: time for...      BE234       1  \n",
       "5706118  Psychosocial impact of sickle cell disorder: p...      UKI72       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slice the data to only include data from 2010 to 2021\n",
    "df = df[(df['pubyear'] >= 2010) & (df['pubyear'] <= 2021)]\n",
    "df['pubyear'].unique()\n",
    "\n",
    "# create 'period' column  \n",
    "def assign_period(pubyear):\n",
    "    if 2010 <= pubyear <= 2011:\n",
    "        return 1\n",
    "    elif 2012 <= pubyear <= 2013:\n",
    "        return 2\n",
    "    elif 2014 <= pubyear <= 2015:\n",
    "        return 3\n",
    "    elif 2016 <= pubyear <= 2017:\n",
    "        return 4\n",
    "    elif 2018 <= pubyear <= 2019:\n",
    "        return 5\n",
    "    elif 2020 <= pubyear <= 2021:\n",
    "        return 6\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['period'] = df['pubyear'].apply(assign_period)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset into quant and non-quant publications \n",
    "quant = df[df['quant_label'] == 'quant']\n",
    "non_quant = df[df['quant_label'] != 'quant']\n",
    "\n",
    "# remove '\\r\\n<p>' from the abstracts \n",
    "quant['abstract'] = quant['abstract'].str.replace('\\r\\n<p>', '')\n",
    "non_quant['abstract'] = non_quant['abstract'].str.replace('\\r\\n<p>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubid</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>quant_label</th>\n",
       "      <th>abstract</th>\n",
       "      <th>itemtitle</th>\n",
       "      <th>eu_nuts_id</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5707039</th>\n",
       "      <td>7280031.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>quant</td>\n",
       "      <td>In this paper we present a new formalism for q...</td>\n",
       "      <td>A new approach to modelling quantum distributi...</td>\n",
       "      <td>UKM34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5708613</th>\n",
       "      <td>7308386.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>quant</td>\n",
       "      <td>In the paper it is shown that every physically...</td>\n",
       "      <td>Unification of Two Approaches to Quantum Logic...</td>\n",
       "      <td>PL633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5708693</th>\n",
       "      <td>7308665.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>quant</td>\n",
       "      <td>This paper introduces a short survey on recent...</td>\n",
       "      <td>Notes on the Essential System to Acquire Infor...</td>\n",
       "      <td>ITI43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5708810</th>\n",
       "      <td>7319734.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>quant</td>\n",
       "      <td>We experimentally demonstrate a detection sche...</td>\n",
       "      <td>Highly Efficient State-Selective Submicrosecon...</td>\n",
       "      <td>DE212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5708811</th>\n",
       "      <td>7319734.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>quant</td>\n",
       "      <td>We experimentally demonstrate a detection sche...</td>\n",
       "      <td>Highly Efficient State-Selective Submicrosecon...</td>\n",
       "      <td>DE21H</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17570341</th>\n",
       "      <td>46606846.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>quant</td>\n",
       "      <td>Europium sulfide (EuS) thin films are appealin...</td>\n",
       "      <td>Ferromagnetic Europium Sulfide Thin Films: Inf...</td>\n",
       "      <td>DEA51</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17570398</th>\n",
       "      <td>46606985.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>quant</td>\n",
       "      <td>The discovery of superconductivity in the heav...</td>\n",
       "      <td>Magnetic reshuffling and feedback on supercond...</td>\n",
       "      <td>FR714</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17570399</th>\n",
       "      <td>46606985.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>quant</td>\n",
       "      <td>The discovery of superconductivity in the heav...</td>\n",
       "      <td>Magnetic reshuffling and feedback on supercond...</td>\n",
       "      <td>CZ010</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17570400</th>\n",
       "      <td>46606985.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>quant</td>\n",
       "      <td>The discovery of superconductivity in the heav...</td>\n",
       "      <td>Magnetic reshuffling and feedback on supercond...</td>\n",
       "      <td>FR623</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17570736</th>\n",
       "      <td>46607582.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>quant</td>\n",
       "      <td>Semiconductor nanoplatelets (NPLs), with their...</td>\n",
       "      <td>Dark and Bright Excitons in Halide Perovskite ...</td>\n",
       "      <td>DE212</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46432 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pubid  pubyear quant_label  \\\n",
       "5707039    7280031.0     2010       quant   \n",
       "5708613    7308386.0     2010       quant   \n",
       "5708693    7308665.0     2010       quant   \n",
       "5708810    7319734.0     2010       quant   \n",
       "5708811    7319734.0     2010       quant   \n",
       "...              ...      ...         ...   \n",
       "17570341  46606846.0     2021       quant   \n",
       "17570398  46606985.0     2021       quant   \n",
       "17570399  46606985.0     2021       quant   \n",
       "17570400  46606985.0     2021       quant   \n",
       "17570736  46607582.0     2021       quant   \n",
       "\n",
       "                                                   abstract  \\\n",
       "5707039   In this paper we present a new formalism for q...   \n",
       "5708613   In the paper it is shown that every physically...   \n",
       "5708693   This paper introduces a short survey on recent...   \n",
       "5708810   We experimentally demonstrate a detection sche...   \n",
       "5708811   We experimentally demonstrate a detection sche...   \n",
       "...                                                     ...   \n",
       "17570341  Europium sulfide (EuS) thin films are appealin...   \n",
       "17570398  The discovery of superconductivity in the heav...   \n",
       "17570399  The discovery of superconductivity in the heav...   \n",
       "17570400  The discovery of superconductivity in the heav...   \n",
       "17570736  Semiconductor nanoplatelets (NPLs), with their...   \n",
       "\n",
       "                                                  itemtitle eu_nuts_id  period  \n",
       "5707039   A new approach to modelling quantum distributi...      UKM34       1  \n",
       "5708613   Unification of Two Approaches to Quantum Logic...      PL633       1  \n",
       "5708693   Notes on the Essential System to Acquire Infor...      ITI43       1  \n",
       "5708810   Highly Efficient State-Selective Submicrosecon...      DE212       1  \n",
       "5708811   Highly Efficient State-Selective Submicrosecon...      DE21H       1  \n",
       "...                                                     ...        ...     ...  \n",
       "17570341  Ferromagnetic Europium Sulfide Thin Films: Inf...      DEA51       6  \n",
       "17570398  Magnetic reshuffling and feedback on supercond...      FR714       6  \n",
       "17570399  Magnetic reshuffling and feedback on supercond...      CZ010       6  \n",
       "17570400  Magnetic reshuffling and feedback on supercond...      FR623       6  \n",
       "17570736  Dark and Bright Excitons in Halide Perovskite ...      DE212       6  \n",
       "\n",
       "[46432 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by region, count the number of unique 'pubid' \n",
    "quant_grouped = quant.groupby('eu_nuts_id').agg({'pubid': 'nunique'}).reset_index()\n",
    "quant_grouped = quant_grouped.rename(columns={'pubid': 'count'})\n",
    "\n",
    "# check the top 100 regions with the most publications\n",
    "quant_grouped = quant_grouped.sort_values(by='count', ascending=False)\n",
    "\n",
    "# slice 'quant' dataframe to only include the top 50 regions with the most publications\n",
    "top_100 = quant_grouped.head(100)\n",
    "top_100 = top_100['eu_nuts_id'].tolist()\n",
    "quant_top_100 = quant[quant['eu_nuts_id'].isin(top_100)]\n",
    "quant_top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice 'quant' dataframe to only include the top 50 regions with the most publications\n",
    "top_100 = quant_grouped.head(100)\n",
    "top_100 = top_100['eu_nuts_id'].tolist()\n",
    "quant_top_100 = quant[quant['eu_nuts_id'].isin(top_100)]\n",
    "non_quant_top_100 = non_quant[non_quant['eu_nuts_id'].isin(top_100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom stop words\n",
    "custom_stop_words = [\"quantum\", \"determining\", \"method\", \"includes\", \"based\", \"second\", \"using\", 'showed', 'problem',\n",
    "                     \"study\", \"non\", \"entanglement\", \"qubit\", # quantum-related words\n",
    "                     \"12\", \"14\", \"15\", \"2014\", \"iii\", \"ii\", \"10\", \"1a\", \"11\", \"cm\", \"60\", '28', '250', '17',\n",
    "                     \"25\", \"40\", \"model\", \"2005\", \"ar\", \"rh\", \"kv\", \"sb\", \"ac\", \"www\", \"nm\", 'srr', 'xy', 'sigma',\n",
    "                     \"mo\", \"nm\", \"18\", \"ee\", \"pb\", \"gev\", \"mu\", \"rh\", \"ln\", \"a15\", \"13\", \"ag\", '55', 'effect','nd3',\n",
    "                     \"nm\", \"qds\", \"iv\", \"ag\", \"rms\", \"le\", \"p53\", \"pl\", \"fe\", \"ir\", \"4abn\", \"nmr\", \"ni\", '05', '74',\n",
    "                     \"ta\", \"tio2\", \"thz\", \"db\", \"hz\", \"iso\", \"pss\", \"1310\", \"nu\", \"tf\", \"tf\", '33', '350',\n",
    "                     \"km\", \"tev\", \"gev\", \"fb\", \"tl\", \"t2dm\", \"nlc\", \"dot\", \"si\", \"cb7\", \"mc\", \"soi\", '100', \n",
    "                     \"dots\", \"pi\", \"qd\", \"la\", \"cdw\", \"iqp\", \"ccl4\", \"no2\", \"mm\", \"p1\", \"bi\", \"ho\", \"rs\", '00',\n",
    "                     \"qh\", \"tio2\", \"nu\", \"nir\", \"time\", \"bm12\", \"ssi\", \"u6\", \"current\", \"nmr\", 'hs', 'sp',\n",
    "                     \"gev\", \"tot\", \"1h\", \"hh\", \"hi\", \"qds\", \"pss\", \"no2\", \"zb\", \"wz\", \"al\", \"ge\", 'ff', 'der',\n",
    "                     \"ect\", \"mdd\", \"95\", \"girls\", \"uc\", \"cpe\", \"di\", \"uc\", \"er3\", \"new\", 's190', 's208', '44',\n",
    "                     \"od\", \"ch3\", \"yb\", \"eu\", \"tb\", \"nr\", \"ope3\", \"4tp\", \"mu\", \"center\", \"la\", '3b', 'eag1',\n",
    "                     \"number\", \"su\", \"ch4\", \"fe\", \"cr\", \"ni\", \"eat\", \"ws\", \"spp\", \"mn2\", \"mm\", 'ss', 'oc',\n",
    "                     \"rf\", \"li\", \"2016\", \"16\", \"sic\", \"si\", \"von\", \"sept3\", \"0d\", \"36\", 'igf1r', 'ma', 'mnc', ''\n",
    "                     'o3', 'correctly', 'tb', 'te', 'il', 'cd39', 'cd73', 'au', 'ou', 'ml', 'mg', 'puo2', '298',\n",
    "                     'sec', 'eta', 'ba122', 'al', 'cgm', 'h2s', 'ir', 'sni', '45', '75', 'tio2', '60', 'sp', '4000', \n",
    "                     'ge', 'si', '5p', '120', '2d', 'random', 'theory', 'li', 'qds', 'qd', 'p3ht', 'mu', '69', '71',\n",
    "                     'p2', 'gw', '511', '3885', 'yb3', '25', 'sir', '240', 'ro', 't34', '10', '6p', 'bbb', 'c1', 'c3',\n",
    "                     'nno', 'nlo', 'ag', 'sb', 'naf', 'ga', 'se', 'cu', 'gd', 'pf6', 'dots', 'dat', '4f', \n",
    "                     'ch3', 'center', 'centre', 'mc3t3', 'e1', '1064', 'jc', 'ga', 'cf3', 'bu', 'zn', 'near', \n",
    "                     'np', 'zn', 'cp', 'fr', 'nmc', 'nca', 'oco', 'dsb', 'ssb', 'oe', 'mn', '300', '80', 'b12x12',\n",
    "                     'nh', 'br', 'mw', '77', 'ig', '24', 'sigma', '700', 'ij', '1h', 'approach', '4b', '343', \n",
    "                     's6', 'ce3', 'advantages', 'children', 'phase', '001', 'ad', 'dy3', '111', '2000', 'matter', '99',\n",
    "                     'results', 'used', 'methods', 'analysis', 'used', 'associated', 'different', 'argue', 'article', \n",
    "                     'let', 'research', 'data', 'paper', 'examines', 'prove', 'proposed', 'samples', 'study', 'similar', \n",
    "                     'compared', 'significantly', 'high', 'publication', 'significant', 'use', 'studies', 'category', \n",
    "                     'use', 'field', 'group', 'years', '000', '2024', 'field', 'risk', 'non', 'age'\n",
    "                     ]  \n",
    "\n",
    "# add stop words \n",
    "default_stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "# combine the two stop words lists\n",
    "all_stop_words = list(default_stop_words.union(custom_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "# Create an empty list to store the results\n",
    "all_results = []\n",
    "\n",
    "# Iterate over each 'eu_nuts_id' and 'period', showing progress with tqdm\n",
    "for nuts in tqdm(quant_top_100['eu_nuts_id'].unique(), desc=\"Processing NUTS regions\"):\n",
    "    for i in range(1, 7):\n",
    "        # Create a filtered copy of the dataset for 'eu_nuts_id' and 'period'\n",
    "        filtered_quant = quant_top_100[(quant_top_100['eu_nuts_id'] == nuts) & (quant_top_100['period'] == i)].copy()\n",
    "\n",
    "        # Extract the 'abstract' column for BERTopic modeling, and ensure all entries are strings\n",
    "        docs = filtered_quant['abstract'].dropna().astype(str).tolist()\n",
    "\n",
    "        if len(docs) == 0:\n",
    "            print(f\"No valid documents for NUTS region {nuts} and period {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Term Frequency, Inverse Document Frequency & transform docs into vectors\n",
    "            vectorizer_model = CountVectorizer(stop_words=all_stop_words)\n",
    "            ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "            embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "            embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "            # Dynamically set the number of neighbors for UMAP based on the number of documents\n",
    "            n_neighbors = min(5, len(docs) - 1)  # Ensures n_neighbors <= number of documents\n",
    "            if n_neighbors <= 1:  # UMAP requires n_neighbors to be at least 2\n",
    "                print(f\"Skipping NUTS region {nuts} and period {i} due to insufficient documents.\")\n",
    "                continue\n",
    "\n",
    "            umap_model = UMAP(n_neighbors=n_neighbors, min_dist=0.0, metric='cosine')\n",
    "\n",
    "            # hyper parameter 설정, 최대 10개의 주제를, 최소 min_topic_sizes 보다 큰 클러스터로 생성 \n",
    "            n_gram_ranges = (1, 1)\n",
    "            min_topic_sizes = max(2, int(len(docs) * 0.015))  # 한 클러스터에 포함되는 최소 문서 수  (1, 1.5, 2, 3%)\n",
    "            nr_topics_options = 10  # 최대 클러스터링 수 (5, 10, 15, 20개)\n",
    "\n",
    "            topic_model = BERTopic(\n",
    "                n_gram_range=n_gram_ranges,\n",
    "                min_topic_size=min_topic_sizes,\n",
    "                nr_topics=nr_topics_options,\n",
    "                embedding_model=embedding_model,\n",
    "                vectorizer_model=vectorizer_model,\n",
    "                calculate_probabilities=True,\n",
    "                ctfidf_model=ctfidf_model,\n",
    "                umap_model=umap_model\n",
    "            )\n",
    "\n",
    "            topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "            freq = topic_model.get_topic_info()\n",
    "\n",
    "            # Add 'eu_nuts_id' and 'period' columns to the topic info DataFrame\n",
    "            freq['eu_nuts_id'] = nuts\n",
    "            freq['period'] = i\n",
    "\n",
    "            # Append the result to the list\n",
    "            all_results.append(freq)\n",
    "\n",
    "            print(f\"Processed topic information for NUTS region {nuts} and period {i}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing NUTS region {nuts} and period {i}: {e}\")\n",
    "\n",
    "# Combine all the results into a single DataFrame\n",
    "quantum_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Define a function to extract only the numbered labels\n",
    "def extract_labels(text):\n",
    "    # Use regex to extract only lines that start with a number followed by a period (e.g., \"1. \")\n",
    "    labels = re.findall(r'^\\d+\\.\\s.+', text, re.MULTILINE)\n",
    "    # Join the extracted labels back into a single string\n",
    "    return \"\\n\".join(labels)\n",
    "\n",
    "# Apply the function to the 'content' column\n",
    "quantum_results['content'] = quantum_results['content'].apply(extract_labels)\n",
    "\n",
    "# Save the final combined DataFrame to a single CSV file\n",
    "quantum_results.to_csv('quantum_pub_bertopic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the results\n",
    "all_results = []\n",
    "\n",
    "# Iterate over each 'eu_nuts_id' and 'period', showing progress with tqdm\n",
    "for nuts in tqdm(non_quant_top_100['eu_nuts_id'].unique(), desc=\"Processing NUTS regions\"):\n",
    "    for i in range(1, 7):\n",
    "        # Create a filtered copy of the dataset for 'eu_nuts_id' and 'period'\n",
    "        filtered_quant = non_quant_top_100[(non_quant_top_100['eu_nuts_id'] == nuts) & (non_quant_top_100['period'] == i)].copy()\n",
    "\n",
    "        # Extract the 'abstract' column for BERTopic modeling, and ensure all entries are strings\n",
    "        docs = filtered_quant['abstract'].dropna().astype(str).tolist()\n",
    "\n",
    "        if len(docs) == 0:\n",
    "            print(f\"No valid documents for NUTS region {nuts} and period {i}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Term Frequency, Inverse Document Frequency & transform docs into vectors\n",
    "            vectorizer_model = CountVectorizer(stop_words=all_stop_words)\n",
    "            ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "            embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "            embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "            # Dynamically set the number of neighbors for UMAP based on the number of documents\n",
    "            n_neighbors = min(10, len(docs) - 1)  # Ensures n_neighbors <= number of documents\n",
    "            if n_neighbors <= 1:  # UMAP requires n_neighbors to be at least 2\n",
    "                print(f\"Skipping NUTS region {nuts} and period {i} due to insufficient documents.\")\n",
    "                continue\n",
    "\n",
    "            umap_model = UMAP(n_neighbors=n_neighbors, min_dist=0.0, metric='cosine') # 0.0-0.3, a lower min_dist makes the clusters tighgter, whereas a higher value makes dispersed clusters\n",
    "\n",
    "            # hyper parameter 설정, 최대 10개의 주제를, 최소 min_topic_sizes 보다 큰 클러스터로 생성 \n",
    "            n_gram_ranges = (1, 1)\n",
    "            min_topic_sizes = max(2, int(len(docs) * 0.01))  # 한 클러스터에 포함되는 최소 문서 수  (1, 1.5, 2, 3%)\n",
    "            nr_topics_options = 40  # 최대 클러스터링 수 (5, 10, 15, 20개)\n",
    "\n",
    "            # 문서별로 할당된 주제 topics와 확률 probs\n",
    "            topic_model = BERTopic(\n",
    "                n_gram_range=n_gram_ranges,\n",
    "                min_topic_size=min_topic_sizes,\n",
    "                nr_topics=nr_topics_options,\n",
    "                embedding_model=embedding_model,\n",
    "                vectorizer_model=vectorizer_model,\n",
    "                calculate_probabilities=True,\n",
    "                ctfidf_model=ctfidf_model,\n",
    "                umap_model=umap_model\n",
    "            )\n",
    "\n",
    "            topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "            # 결과를 저장\n",
    "            freq = topic_model.get_topic_info()\n",
    "\n",
    "            # Add 'eu_nuts_id' and 'period' columns to the topic info DataFrame\n",
    "            freq['eu_nuts_id'] = nuts\n",
    "            freq['period'] = i\n",
    "\n",
    "            # Append the result to the list\n",
    "            all_results.append(freq)\n",
    "\n",
    "            print(f\"Processed topic information for NUTS region {nuts} and period {i}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing NUTS region {nuts} and period {i}: {e}\")\n",
    "\n",
    "# Combine all the results into a single DataFrame\n",
    "non_quant_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Define a function to extract only the numbered labels\n",
    "def extract_labels(text):\n",
    "    # Use regex to extract only lines that start with a number followed by a period (e.g., \"1. \")\n",
    "    labels = re.findall(r'^\\d+\\.\\s.+', text, re.MULTILINE)\n",
    "    # Join the extracted labels back into a single string\n",
    "    return \"\\n\".join(labels)\n",
    "\n",
    "# Apply the function to the 'content' column\n",
    "non_quant_results['content'] = non_quant_results['content'].apply(extract_labels)\n",
    "\n",
    "# Save the final DataFrame to a single CSV file\n",
    "non_quant_results.to_csv('pub_bertopic.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
