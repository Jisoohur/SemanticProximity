{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Proximity Research\n",
    "- Made by: Jisoo Hur (Ph.D.) & Keungoui Kim (Ph.D.)\n",
    "- Goal: 05. BERT-based Similarity Measurement\n",
    "- Data set: WoS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"H:/GD_awekimm/[HGU]/[Research]/12_허지수/00_SemanticProximity/SemanticProximity_research/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the datasets\n",
    "pub_data = pd.read_csv(dir+'pub_bertopic_chatgpt.csv')\n",
    "quantum_data = pd.read_csv(dir+'quantum_pub_bertopic_chatgpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eu_nuts_id</th>\n",
       "      <th>period</th>\n",
       "      <th>keyword</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['nomenclature', 'names', 'botanical', 'intern...</td>\n",
       "      <td>International Botanical Nomenclature and Class...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['galaxies', 'star', 'dust', 'galaxy', 'redshi...</td>\n",
       "      <td>Infrared Emission and Stellar Mass in Distant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['wireless', 'channel', 'modulation', 'mimo', ...</td>\n",
       "      <td>Advanced Wireless Communication Techniques for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['patients', 'cells', 'cell', 'species', 'expr...</td>\n",
       "      <td>Impact of Increased Cell Expression in Disease...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UKM25</td>\n",
       "      <td>1</td>\n",
       "      <td>['bar', 'detector', 'gamma', 'decays', 'gt', '...</td>\n",
       "      <td>Gamma Decay Detection and Branching Ratios in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  eu_nuts_id  period                                            keyword  \\\n",
       "0      UKM25       1  ['nomenclature', 'names', 'botanical', 'intern...   \n",
       "1      UKM25       1  ['galaxies', 'star', 'dust', 'galaxy', 'redshi...   \n",
       "2      UKM25       1  ['wireless', 'channel', 'modulation', 'mimo', ...   \n",
       "3      UKM25       1  ['patients', 'cells', 'cell', 'species', 'expr...   \n",
       "4      UKM25       1  ['bar', 'detector', 'gamma', 'decays', 'gt', '...   \n",
       "\n",
       "                                             content  \n",
       "0  International Botanical Nomenclature and Class...  \n",
       "1  Infrared Emission and Stellar Mass in Distant ...  \n",
       "2  Advanced Wireless Communication Techniques for...  \n",
       "3  Impact of Increased Cell Expression in Disease...  \n",
       "4  Gamma Decay Detection and Branching Ratios in ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Similairty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    # Get the output from BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Get the embeddings for the [CLS] token (the first token)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "# Function to calculate BERT-based similarity\n",
    "def calculate_bert_similarity(df1, df2, region, period):\n",
    "    # Filter data by region and period\n",
    "    df1_filtered = df1[(df1['eu_nuts_id'] == region) & (df1['period'] == period)]\n",
    "    df2_filtered = df2[(df2['eu_nuts_id'] == region) & (df2['period'] == period)]\n",
    "    \n",
    "    # Check if both filtered datasets have data\n",
    "    if df1_filtered.empty or df2_filtered.empty:\n",
    "        return None  # Return None if there's no data for the given region and period\n",
    "    \n",
    "    # Concatenate the 'content' field to create a document for each set\n",
    "    doc1 = ' '.join(df1_filtered['content'].astype(str)).lower()\n",
    "    doc2 = ' '.join(df2_filtered['content'].astype(str)).lower()\n",
    "    \n",
    "    # Preprocess the documents by removing stopwords\n",
    "    doc1 = remove_stopwords(doc1)\n",
    "    doc2 = remove_stopwords(doc2)\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    embedding1 = get_bert_embedding(doc1)\n",
    "    embedding2 = get_bert_embedding(doc2)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    \n",
    "    return similarity[0][0]  # Return the similarity score\n",
    "\n",
    "# Apply similarity calculation for each unique combination of region and period\n",
    "regions = pub_data['eu_nuts_id'].unique()\n",
    "periods = pub_data['period'].unique()\n",
    "\n",
    "results = []\n",
    "\n",
    "for region in regions:\n",
    "    for period in periods:\n",
    "        bert_score = calculate_bert_similarity(pub_data, quantum_data, region, period)\n",
    "        results.append({'eu_nuts_id': region, 'period': period, 'bert_similarity': bert_score})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "similarity_df = pd.DataFrame(results)\n",
    "\n",
    "# Display or save the results\n",
    "print(similarity_df) \n",
    "\n",
    "similarity_df.to_csv('similarity_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "regions = pub_data['eu_nuts_id'].unique()\n",
    "periods = pub_data['period'].unique()\n",
    "\n",
    "results = []\n",
    "for region in regions:\n",
    "    for period in periods:\n",
    "        \n",
    "        df1_filtered = pub_data[(pub_data['eu_nuts_id'] == region) & (pub_data['period'] == period)]\n",
    "        df2_filtered = quantum_data[(quantum_data['eu_nuts_id'] == region) & (quantum_data['period'] == period)]\n",
    "        \n",
    "        # Not much effective for this case\n",
    "        doc1 = [re.sub(r\"<(.*?)>\", r\"\\1\", item) for item in df1_filtered['content'].to_list()]\n",
    "        doc1 = [item.replace('\"', '') for item in doc1]\n",
    "        doc2 = [re.sub(r\"<(.*?)>\", r\"\\1\", item) for item in df2_filtered['content'].to_list()]\n",
    "        doc2 = [item.replace('\"', '') for item in doc2]\n",
    "\n",
    "        embeddings_a = model.encode(doc1, convert_to_tensor=True)  \n",
    "        embeddings_b = model.encode(doc2, convert_to_tensor=True)  \n",
    "\n",
    "        similarities = []\n",
    "        for embed_a in embeddings_a:  \n",
    "            for embed_b in embeddings_b:  \n",
    "                similarity = util.cos_sim(embed_a, embed_b).item()  \n",
    "                similarities.append(similarity)\n",
    "        average_similarity = np.mean(similarities)\n",
    "\n",
    "        results.append({'eu_nuts_id': region, 'period': period, 'bert_similarity': average_similarity})\n",
    "\n",
    "similarity_df = pd.DataFrame(results)\n",
    "\n",
    "similarity_df.to_csv(dir+'similarity_bert_ed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "region='UKF14'\n",
    "period=4\n",
    "df1_filtered = pub_data[(pub_data['eu_nuts_id'] == region) & (pub_data['period'] == period)]\n",
    "df2_filtered = quantum_data[(quantum_data['eu_nuts_id'] == region) & (quantum_data['period'] == period)]\n",
    "\n",
    "# Not much effective for this case\n",
    "doc1 = [re.sub(r\"<(.*?)>\", r\"\\1\", item) for item in df1_filtered['content'].to_list()]\n",
    "doc1 = [item.replace('\"', '') for item in doc1]\n",
    "doc2 = [re.sub(r\"<(.*?)>\", r\"\\1\", item) for item in df2_filtered['content'].to_list()]\n",
    "doc2 = [item.replace('\"', '') for item in doc2]\n",
    "\n",
    "embeddings_a = model.encode(doc1, convert_to_tensor=True)  # A의 임베딩\n",
    "embeddings_b = model.encode(doc2, convert_to_tensor=True)  # B의 임베딩\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
